{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FitNets-Maxout-Student model.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOIFGL52Tx3RMomsoWbpcRc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aryanasadianuoit/Fitnets/blob/master/FitNets_Maxout_Student_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_s_EkV9e6Uq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Function\n",
        "from dataloader import cifar10\n",
        "from torch.nn import functional as F\n",
        "from torch.nn.parameter import Parameter \n",
        "from torchsummary import summary\n",
        "from general_utils import train\n",
        "from globals import *\n",
        "\n",
        "MAX_OUT_NUMBER = 2\n",
        "W_B_LR_SCALE = 0.05\n",
        "\n",
        "class Maxout(Function):\n",
        "\n",
        "\n",
        "\n",
        "    # Note that both forward and backward are @staticmethods\n",
        "    @staticmethod\n",
        "    # bias is an optional argument\n",
        "    def forward(ctx, input):\n",
        "        x = input\n",
        "          # Maxout Parameter\n",
        "        max_out = MAX_OUT_NUMBER\n",
        "        #print(\"MAX-OUT NUMBER ===>\", max_out_number)\n",
        "        kernels = x.shape[1]  # to get how many kernels/output\n",
        "        feature_maps = int(kernels / max_out)\n",
        "        out_shape = (x.shape[0], feature_maps, max_out, x.shape[2], x.shape[3])\n",
        "        #print(\"OUt shape ==> \", out_shape)\n",
        "        x = x.view(out_shape)\n",
        "        y, indices = torch.max(x[:, :, :], 2)\n",
        "        ctx.save_for_backward(input)\n",
        "        ctx.indices = indices\n",
        "        ctx.max_out = max_out\n",
        "        return y\n",
        "\n",
        "    # This function has only a single output, so it gets only one gradient\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "\n",
        "        input1, indices, max_out = ctx.saved_variables[0], Variable(ctx.indices), ctx.max_out\n",
        "        input = input1.clone()\n",
        "        for i in range(max_out):\n",
        "            a0 = indices == i\n",
        "            input[:, i:input.data.shape[1]:max_out] = a0.float() * grad_output * W_B_LR_SCALE\n",
        "\n",
        "        return input\n",
        "\n",
        "\n",
        "# This is an example for image reconstruction but you can modify it as you want.\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "\n",
        "        self.h0 = nn.Conv2d(3, 32, kernel_size=9, padding=4, stride=1)\n",
        "        self.mo0 = Maxout.apply\n",
        "        #self.mo1 = Maxout.apply\n",
        "\n",
        "\n",
        "        #Maxout(max_out_number=4)\n",
        "        self.h1 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
        "        self.mo1 = Maxout.apply\n",
        "        #self.mo2 = Maxout.apply\n",
        "\n",
        "        self.h2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
        "        self.mo2 = Maxout.apply\n",
        "\n",
        "        self.h3 = nn.Conv2d(16, 48, kernel_size=3, padding=1)\n",
        "        self.mo3 = Maxout.apply\n",
        "       \n",
        "\n",
        "        self.h4 = nn.Conv2d(24, 48, kernel_size=3, padding=1)\n",
        "        self.h4_pooling = self.pool = nn.MaxPool2d(kernel_size=2,stride=2)\n",
        "        self.mo4 = Maxout.apply\n",
        "\n",
        "        self.h5 = nn.Conv2d(24, 80, kernel_size=3, padding=1)\n",
        "        self.mo5 = Maxout.apply\n",
        "\n",
        "        self.h6 = nn.Conv2d(40, 80, kernel_size=3, padding=1)\n",
        "        self.mo6 = Maxout.apply\n",
        "\n",
        "        self.h7 = nn.Conv2d(40, 80, kernel_size=3, padding=1)\n",
        "        self.mo7 = Maxout.apply\n",
        "\n",
        "        self.h8 = nn.Conv2d(40, 80, kernel_size=3, padding=1)\n",
        "        self.mo8 = Maxout.apply\n",
        "\n",
        "        self.h9 = nn.Conv2d(40, 80, kernel_size=3, padding=1)\n",
        "        self.mo9 = Maxout.apply\n",
        "\n",
        "        self.h10 = nn.Conv2d(40, 80, kernel_size=3, padding=1)\n",
        "        self.h10_pooling = self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.mo10 = Maxout.apply\n",
        "\n",
        "        self.h11 = nn.Conv2d(40, 128, kernel_size=3, padding=1)\n",
        "        self.mo11 = Maxout.apply\n",
        "\n",
        "        self.h12 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.mo12 = Maxout.apply\n",
        "\n",
        "        self.h13 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.mo13 = Maxout.apply\n",
        "\n",
        "        self.h14 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.mo14 = Maxout.apply\n",
        "\n",
        "        self.h15 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.mo15 = Maxout.apply\n",
        "\n",
        "        self.h16 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.h16_pooling = self.pool = nn.MaxPool2d(kernel_size=8, stride=1)\n",
        "        self.mo16 = Maxout.apply\n",
        "\n",
        "\n",
        "        self.fc1 = nn.Linear(in_features= 64 * 1 * 1, out_features=10)\n",
        "   \n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.mo0(self.h0(x))\n",
        "        out = self.mo1(self.h1(out))\n",
        "        out = self.mo2(self.h2(out))\n",
        "        out = self.mo3(self.h3(out))\n",
        "        out = self.mo4(self.h4_pooling(self.h4(out)))\n",
        "        out = self.mo5(self.h5(out))\n",
        "        out = self.mo6(self.h6(out))\n",
        "        out = self.mo7(self.h7(out))\n",
        "        out = self.mo8(self.h8(out))\n",
        "        out = self.mo9(self.h9(out))\n",
        "        out = self.mo10(self.h10_pooling(self.h10(out)))\n",
        "        out = self.mo11(self.h11(out))\n",
        "        out = self.mo12(self.h12(out))\n",
        "        out = self.mo13(self.h13(out))\n",
        "        out = self.mo14(self.h14(out))\n",
        "        out = self.mo15(self.h15(out))\n",
        "        out = self.mo16(self.h16_pooling(self.h16(out)))\n",
        "        out = out.view(-1, 1 * 1 * 64 )\n",
        "        out = F.relu(self.fc1(out))\n",
        "        #out = F.relu(self.fc2(out))\n",
        "       # out = self.fc3(out)\n",
        "        out = F.softmax(out, dim=-1)\n",
        "        return out\n",
        "\n",
        "test = CNN()\n",
        "print(test)\n",
        "\n",
        "\n",
        "summary(model=test, input_size=(3,32,32), device=\"cpu\")\n",
        "\n",
        "train_loader, test_loader = cifar10()\n",
        "optimizer = torch.optim.SGD(test.parameters(),lr= 0.005, momentum= 0.9,weight_decay= 0.9)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "train(test,optimizer= optimizer,criterion= criterion, train_loder= train_loader, test_loader= test_loader, train_on_gpu= True, multiple_gpu= True,\n",
        "      path= SERVER_2_PREFIX_ADDRESS+\"testmaxout.pth\", epochs= 5)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}